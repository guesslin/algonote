<html lang="zh-TW"><head><meta charset="UTF-8" /><link rel="stylesheet" href="style.css" />
<title>演算法筆記 - Equation Solving</title></head><body>
<div class="a"><div class="h">
<p class="b">Equation Solving</p>
</div><div class="c">
<p class="t">Equation</p>
<p>兩個相等的變數式子，就叫作「方程式」。方程這兩字借自中國古代數學書籍；以現代眼光來看，這個名稱不太精準。按照英文字面意義來翻譯，應該稱作「等式」才對。</p>
<pre>
2x + 1 = (4x² + 3) / (x - 1)
</pre>
<p>中學數學教了很多方程式的計算，例如解一元二次方程式、解聯立方程式，並且搭配幾何學一起講解。大學指考前，算了千百次，應當難不倒各位。</p>
<p>方程式有什麼用途呢？其實就是設x、解x。這兩件事情很難用中文講個明白，不過只要經歷許多數學題目之後，自然而然就能體會到設x、解x的精神所在。</p>
<img src="findx.png">
<p>方程式通常都會重新整理成函數的模樣，方便求解。</p>
<pre>
2x + 1 = (4x² + 3) / (x - 1)

等量減法公理，兩邊同減一樣多的東西。（移項）
2x + 1 - (4x² + 3) / (x - 1) = 0

整理成函數的模樣，然後求根即可！
f(x) = 2x + 1 - (4x² + 3) / (x - 1)
</pre>
<p class="t">System of Equations（Simultaneous Equations）</p>
<p>許多道方程式同時成立，整體稱作「方程組」或者「聯立方程式」。</p>
<pre>
{ 1 x + 2 y - 4 sin(z) = 1
{ 2 x - 4 y + 2 cos(z) = 2
{ 3 x + 1 y + 1 exp(z) = 3
</pre>
<p class="t">求根與求解</p>
<p>求根跟求解本質上是同一件事情，求根與求解的演算法互通。</p>
<p>一元函數習慣求根，二元以上函數習慣求解。我也不知道原因。</p>

</div></div><div class="a"><div class="h">
<p class="b">Linear Equation Solving: LU Decomposition（Gaussian Elimination）</p>
</div><div class="c">
<p class="t">Linear Equation</p>
<p>「線性方程式」。方程式，僅由變數的加法、變數的倍率組成。</p>
<pre>
1 x + 2 y - 5 z = 5
</pre>
<p class="t">System of Linear Equations</p>
<p>「線性方程組」。許多道線性方程式同時成立。</p>
<pre>
{ 1 x + 2 y - 5 z = 5
{ 2 x + 4 y + 6 z = 1
{ 3 x + 1 y + 7 z = 4
</pre>
<p>線性方程組可以寫成矩陣乘法的形式。</p>
<pre>
{ 1 x + 2 y - 5 z = 5        [ 1 2 -5 ] [ x ]   [ 5 ]
{ 2 x + 4 y + 6 z = 1  ===>  [ 2 4  6 ] [ y ] = [ 1 ]
{ 3 x + 1 y + 7 z = 4        [ 3 1  7 ] [ z ]   [ 4 ]
                                 A        x⃗       y⃗
</pre>
<p class="t">Linear Equation與等量公理</p>
<p>這裡預設大家已經相當熟悉線性方程組的計算手法：利用等量公理，由上往下把變數消光光，變成階梯狀；然後由下往上解出每個變數。還不太熟悉的讀者，先回憶一下吧！這個計算手法就叫做「高斯消去法」。</p>
<iframe src="http://www.youtube.com/embed/E3smq4ujIxs"></iframe>
<p class="t">Gaussian Elimination</p>
<p>現在，以矩陣的相關術語，重新解釋「高斯消去法」。</p>
<p>把一個矩陣，化成對角線元素皆為一的上三角矩陣。</p>
<img src="GaussianElimination1.png">
<p>按照字典順序窮舉一對一對的row，每窮舉出一對row，就處理這兩個row──求出首項係數的倍率，以上方row消去下方row，使下方row的首項係數變成零。</p>
<p>有一個特殊情況是，當上方row的首項係數是零的時候，就要考慮與下方row交換，讓上方row的首項係數盡量不是零。</p>
<p>這個交換row的動作稱作pivoting，不為零的那一個首項係數稱作pivot，包含pivot的那一個row稱作pivot row。</p>
<p>高斯消去法的過程，以row的角度來看，只有三種row運算：倍率、相減、交換。但是實作時，我們通常不會特地寫一個row的資料結構、定義這三種運算，因為程式結構太過複雜的話，程式執行時間也會變長。實作時，通常是自己慢慢數索引值，小心的從二維陣列中取得元素，逐步完成row運算。</p>
<p>時間複雜度是O(N^2 * M)，N×M為矩陣的大小。由於一般情況都是討論方陣較多，N與M相等，所以會把時間複雜度寫成O(N^3)。</p>
<p>下面提供方陣的高斯消去法程式碼；至於一般矩陣的高斯消去法，就留給大家自行練習。</p>
<textarea>
double matrix[10][10];

void gaussian_elimination()
{
	for (int i=0; i<10; ++i)
	{
		// 如果上方row的首項係數為零，則考慮與下方row交換。
		if (matrix[i][i] == 0)
			for (int j=i+1; j<10; ++j)
				if (matrix[j][i] != 0)
				{
					// 交換上方row與下方row。
					for (int k=i; k<10; ++k)
						swap(matrix[i][k], matrix[j][k]);
					break;
				}

		if (matrix[i][i] == 0) continue;

		// 上方row的首項係數調整成一。目的是為了讓對角線皆為一。
		double t = matrix[i][i];	// 首項係數
		for (int k=i; k<10; ++k)
			matrix[i][k] /= t;

		// 窮舉並消去下方row。
		for (int j=i+1; j<10; ++j)
			if (matrix[j][i] != 0)
			{
				double t = matrix[j][i];
				for (int k=i; k<10; ++k)
					matrix[j][k] -= matrix[i][k] * t;
			}
	}
}
</textarea>
<p class="t">解線性方程組</p>
<p>矩陣參數化、完成高斯消去法之後，使用Iterative Method，從最後一個row開始，把目前解出的未知數反覆代入到上一個row，求得每一個未知數。</p>
<img src="GaussianElimination2.png">
<p>這個計算過程，是從最後一個未知數開始計算，而不是從第一個未知數開始計算，故命名為「逆向代入back substitution」。逆向代入的時間複雜度是O(N^2)。</p>
<p>如果要讓逆向代入的誤差變小，可以在進行高斯消去法的時候，總是把首項係數絕對值最大的row，挪到最上方，再消去餘下的row。</p>
<img src="GaussianElimination3.png">
<textarea>
double matrix[10][10+1];	// 參數化的矩陣
double x[10];				// 線性方程組的解

void back_substitution()
{
	for (int i=10-1; i>=0; --i)
	{
		double t = 0;
		for (int k=i+1; k<10; ++k)
			t += matrix[i][k] * x[k];
		x[i] = (matrix[i][10] - t) / matrix[i][i];
	}
}
</textarea>
<p class="e">UVa 10109 10524 10828 ICPC 3563</p>
<p class="t">Gauss-Jordan Elimination</p>
<p>「高斯喬登消去法」是延伸版本。把原矩陣的對角線化成一、其餘元素化為零。</p>
<img src="GaussianElimination5.png">
<p>時間複雜度與高斯消去法相同，仍是O(N^3)。</p>
<p>解線性方程組，論效率，高斯消去法是比較好的選擇：高斯消去法暨逆向代入的總步驟數，比高斯喬登消去法還要少。論程式碼長度，高斯喬登消去法是比較好的選擇：只消修改一下高斯消去法的消去範圍，即可得到解，而不必逆向代入。</p>
<p class="t">LUP Decomposition</p>
<p><a href="http://ccjou.wordpress.com/2010/09/01/">http://ccjou.wordpress.com/2010/09/01/</a></p>
<p>「LUP分解」是利用高斯消去法，將一個方陣化為下三角矩陣L、上三角矩陣U、列交換矩陣P，三者的乘積。時間複雜度為O(N^3)。</p>
<p>有時候列交換矩陣P恰好等於單位矩陣I，而不需要把列交換矩陣P寫下來，此時「LUP分解」可簡單稱作「LU分解」。</p>
<p>LUP分解的最大特色是解線性方程組Ax = b，當A固定，b有許多組要解，每次求解僅需時O(N^2)。若是單純地使用高斯消去法，針對每一組不同的b，每次求解皆需時O(N^3)。</p>
<pre>
一、row交換矩陣，調換參數向量的維度順序。
二、下三角矩陣，順向代入（forward substitution）。
三、上三角矩陣，逆向代入（back substitution）。
</pre>
<p class="t">計算一個方陣的inverse</p>
<p>一般是利用「高斯喬登消去法」。</p>
<p>高斯消去法最初的用途是解線性方程組。線性代數開始流行之後，才用來計算矩陣的inverse和determinant。</p>
<img src="GaussianElimination6.png">
<textarea>
double matrix[10][10*2];	// 參數化的矩陣

bool gauss_jordan_elimination()
{
	// 填好參數化的部分
	for (int i=0; i<10; ++i)
		for (int j=0; j<10; ++j)
			matrix[i][10+j] = 0;

	for (int i=0; i<10; ++i)
		matrix[i][10+i] = 1;

	// 開始進行高斯喬登消去法
	// 內容幾乎與高斯消去法相同
	for (int i=0; i<10; ++i)
	{
		if (matrix[i][i] == 0)
			for (int j=i+1; j<10; ++j)
				if (matrix[j][i] != 0)
				{
					for (int k=i; k<10*2; ++k)
						swap(matrix[i][k], matrix[j][k]);
					break;
				}

		// 反矩陣不存在。
		if (matrix[i][i] == 0) return false;

		double t = matrix[i][i];
		for (int k=i; k<10*2; ++k)
			matrix[i][k] /= t;

		// 消去時，所有的row都消去。
		for (int j=0; j<10; ++j)
			if (i != j && matrix[j][i] != 0)
			{
				double t = matrix[j][i];
				for (int k=i; k<10*2; ++k)
					matrix[j][k] -= matrix[i][k] * t;
			}
	}
	return true;
}
</textarea>
<p class="t">計算一個方陣的determinant</p>
<p>一般是利用「高斯消去法」。</p>
<p>進行高斯消去法的過程當中，保留pivot row的原有倍率。最後的上三角矩陣，其對角線元素的乘積，便是determinant。</p>
<img src="GaussianElimination4.png">
<p>如果矩陣裡都是整數，那麼determinant也會是整數。要避免浮點數誤差，可以使用輾轉相除法進行消去。時間複雜度是O(N^3 * logC)，C是過程當中，絕對值最大的首項係數。</p>
<textarea>
int matrix[10][10];

void rowgcd(int* a, int* b, int i)
{
	if (abs(a[i]) < abs(b[i]))
		swap(a, b);

	while (b[i] != 0)
	{
		int t = a[i] / b[i];
		for (int k=i; k<10; ++k)
			a[k] -= b[k] * t;
		swap(a, b);
	}
}

int determinant()
{
	int det = 1;
	for (int i=0; i<10; ++i)
	{
		for (int j=i+1; j<10; ++j)
		{
			// 輾轉相除法進行消去。
			rowgcd(matrix[i], matrix[j], i);

			// 把非零的row，挪到當前的row。
			if (matrix[i][i] == 0 && matrix[j][i] != 0)
			{
				for (int k=i; k<10; ++k)
					swap(matrix[i][k], matrix[j][k]);

				// 兩行交換，determinant會相差一負號。
				det *= -1;
			}
		}

		// determinant等於上三角方陣的對角線乘積。
		det *= matrix[i][i];
		if (det == 0) break;
	}
	return det;
}
</textarea>
<p class="e">UVa 684</p>
<p class="t">延伸閱讀：Cholesky Decomposition</p>
<p>對稱正定矩陣，實施LU分解，L與U剛好互相對稱。</p>
<p>時間複雜度仍為O(N^3)，但是步驟數量較少。</p>

</div></div><div class="a"><div class="h">
<p class="b">Linear Equation Solving: Eigendecomposition</p>
</div><div class="c">
<p class="t">Linear Equations與Linear Transform</p>
<p>線性方程組可以寫成矩陣乘法的形式。</p>
<pre>
{ 1 x + 2 y - 5 z = 5        [ 1 2 -5 ] [ x ]   [ 5 ]
{ 2 x + 4 y + 6 z = 1  ===>  [ 2 4  6 ] [ y ] = [ 1 ]
{ 3 x + 1 y + 7 z = 4        [ 3 1  7 ] [ z ]   [ 4 ]
                                 A        x⃗       y⃗
</pre>
<p>線性方程組得視作線性變換（函數），解線性方程組得視作逆向線性變換（反函數）。</p>
<pre>
                                                 -1
[ 1 2 -5 ] [ x ]   [ 5 ]        [ x ]   [ 1 2 -5 ] [ 5 ]
[ 2 4  6 ] [ y ] = [ 1 ]  ===>  [ y ] = [ 2 4  6 ] [ 1 ]
[ 3 1  7 ] [ z ]   [ 4 ]        [ z ]   [ 3 1  7 ] [ 4 ]
    A        x⃗       y⃗           x⃗         A<sup>-1</sup>        y⃗
</pre>
<p>一旦求得反矩陣，即可輕鬆解線性方程組。</p>
<pre>
solve Ax⃗ = y⃗  ===>  find A<sup>-1</sup>, then x⃗ = A<sup>-1</sup>y⃗
</pre>
<p>計算反矩陣，使用高斯喬登消去法，時間複雜度O(N^3)。</p>
<p>不過與其採用高斯喬登消去法求反矩陣、再用反矩陣解線性方程組，不如直接採用高斯消去法解線性方程組。就當作是學個想法吧。</p>
<p class="t">Eigendecomposition</p>
<p>線性變換的精髓：求得在特徵向量上的分量，分別伸縮，伸縮倍率是特徵值。逆向線性變換的精髓：逆向縮放，縮放倍率變成倒數。</p>
<pre>
             -1          T
A   = Q  Λ  Q   = Q  Λ  Q 

 -1       -1 -1       -1 T
A   = Q  Λ  Q   = Q  Λ  Q 
</pre>
<p>矩陣實施特徵分解，求反矩陣，再拿來解線性方程組。</p>
<p>以特徵值來判斷是否存在反矩陣。特徵值皆不為零，則有反矩陣。就這樣子。</p>
<p>不過與其採用特徵分解求反矩陣、再用反矩陣解線性方程組，不如直接採用高斯消去法解線性方程組。就當作是學個想法吧。</p>

</div></div><div class="a"><div class="h">
<p class="b">Linear Equation Solving: Cramer's Rule</p>
</div><div class="c">
<p class="t">Linear Equations與Geometry</p>
<p>線性方程式得視作幾何元件：點、線、面、……。</p>
<img src="Cramer'sRule1.png">
<!--ContourPlot3D[1 x + 2 y - 5 z == 5, {x, -5, 5}, {y, -5, 5}, {z, -5, 5}, Boxed -> False, Axes -> False, Mesh -> None]-->
<p>線性方程組得視作一堆幾何元件的交集。</p>
<img src="Cramer'sRule2.png">
<!--f := 1 x + 2 y - 5 z - 5; g := 2 x + 4 y + 6 z - 1; h := 3 x + 1 y + 7 z - 4; ContourPlot3D[{f == 0, g == 0, h == 0}, {x, -5, 5}, {y, -5, 5}, {z, -5, 5}, Boxed -> False, Axes -> False, Mesh -> None, ContourStyle -> Directive[Opacity[0.5]]]-->
<p class="t">Cramer's Rule</p>
<p>「<a href="Point.html">兩線交點</a>」的演算法：求得平行四邊形的面積，以面積比例求得交點位置。</p>
<img src="Cramer'sRule3.png">
<p>Cramer's Rule則是此演算法的高維度版本，形成了非常漂亮的公式解！求得超平行體的容積，以容積比例求得解。</p>
<pre>
linear equations:
  Ax⃗ = y⃗

solution:
  { x = det(Aˣ) / det(A)
  { y = det(Aʸ) / det(A)
  { z = det(Aᶻ) / det(A)

unisolvance:
   Ax⃗ = y⃗ has a unique solution   iff   det(A) ≠ 0

example:
  { 1 x + 2 y - 5 z = 5        [ 1 2 -5 ] [ x ]   [ 5 ]
  { 2 x + 4 y + 6 z = 1  ===>  [ 2 4  6 ] [ y ] = [ 1 ]
  { 3 x + 1 y + 7 z = 4        [ 3 1  7 ] [ z ]   [ 4 ]
                                   A        x⃗       y⃗
         _ y⃗                  _ y⃗                  _ y⃗
       [|5| 2 -5 ]       [ 1 |5|-5 ]       [ 1  2 |5|]
  Aˣ = [|1| 4  6 ]  Aʸ = [ 2 |1| 6 ]  Aᶻ = [ 2  4 |1|]
       [|4| 1  7 ]       [ 3 |4| 7 ]       [ 3  1 |4|]
         ‾                    ‾                    ‾
</pre>
<p>determinant是矩陣當中所有向量所構成的超平行體的容積。時間複雜度等於N+1次determinant的時間複雜度，O(N^4)。</p>
<p class="t">Determinant</p>
<p>determinant起初用來判定一個線性方程組是否有解、解是多少，因而稱作「決定因子」。古人沒有意識到determinant是容積。</p>
<p>雖然字面意義是「決定因子」，不過中文教科書譯作「行列式」。真是異想天開的翻譯啊！</p>
<p><a href="http://mathworld.wolfram.com/DeterminantExpansionbyMinors.html">http://mathworld.wolfram.com/DeterminantExpansionbyMinors.html</a></p>
<p>行列式的計算過程是：先刪除一橫行，接著分別刪除每一直行，形成N-1個(N-1)×(N-1)子矩陣，添上正負號。原矩陣的行列式，等於這些子矩陣的行列式總和。每個子矩陣各自遞迴下去，直到N=1。1×1矩陣的行列式，等於矩陣元素。時間複雜度O(N!)。</p>
<p>N = 2 or 3的時候比較特別，可以直接累加所有「左上右下斜線」的乘積、累減「右上左下斜線」的乘積。中學數學課程有教。</p>
<p>計算行列式，也可以使用高斯消去法，時間複雜度O(N^3)。</p>
<p>不過與其採用高斯消去法求行列式、再用行列式解線性方程組，不如直接採用高斯消去法解線性方程組。就當作是學個想法吧。</p>

</div></div><div class="a"><div class="h">
<p class="b">Linear Equation Solving: Preconditioner</p>
</div><div class="c">
<p class="t">Jacobi Preconditioner</p>
<p>用遞推法趨近解答。原理如同「<a href="http://homepage.math.uiowa.edu/~atkinson/ftp/ENA_Materials/Overheads/sec_3-4.pdf">Fixed Point Iteration</a>」。</p>
<pre>
[ 4  3 ] [ x ] = [ 1 ]
[ 2  5 ] [ y ]   [ 2 ]

{ 4x + 3y = 1  => { x = (1 - 3y) / 4
{ 2x + 5y = 2     { y = (2 - 2x) / 5

[ x<sub>0</sub> ] = [ 0 ] 隨便設定一個初始值
[ y<sub>0</sub> ]   [ 0 ]

[ x<sub>1</sub> ] = [ (1 - 3y<sub>0</sub>) / 4 ]
[ y<sub>1</sub> ]   [ (2 - 2x<sub>0</sub>) / 5 ]

[ x<sub>2</sub> ] = [ (1 - 3y<sub>1</sub>) / 4 ]
[ y<sub>2</sub> ]   [ (2 - 2x<sub>1</sub>) / 5 ]
</pre>
<p>三維版本。</p>
<pre>
[ 4  3 -1 ] [ x ]   [ 1 ]
[ 2  5  1 ] [ y ] = [ 2 ]
[-2 -2  6 ] [ z ]   [ 3 ]

{  4x + 3y -  z = 1     { x = (1 - 3y +  z) / 4
{  2x + 5y +  z = 2  => { y = (2 - 2x -  z) / 5
{ -2x - 2y + 6z = 3     { z = (3 + 2x + 2y) / 6

[ x<sub>0</sub> ]   [ 0 ]
[ y<sub>0</sub> ] = [ 0 ] 隨便設定一個初始值
[ z<sub>0</sub> ]   [ 0 ]

[ x<sub>1</sub> ]   [ (1 - 3y<sub>0</sub> +  z<sub>0</sub>) / 4 ]
[ y<sub>1</sub> ] = [ (2 - 2x<sub>0</sub> -  z<sub>0</sub>) / 5 ]
[ z<sub>1</sub> ]   [ (3 + 2x<sub>0</sub> + 2y<sub>0</sub>) / 6 ]
</pre>
<p>任意維度。</p>
<pre>
      Ax = b
(D+L+U)x = b             D是對角線、L是下三角、U是上三角
      Dx = b - (L+U)x
       x = D<sup>-1</sup> [b - (L+U)x]
</pre>
<pre>
x<sub>0</sub> = 隨便設定一個初始值
x<sub>k+1</sub> = D<sup>-1</sup> [b - (L+U)x<sub>k</sub>]
</pre>
<p>時間複雜度是O(N^2 * T)，N是方陣維度，T是遞推次數。</p>
<p>判斷收斂，檢查D<sup>-1</sup>(L+U)的eigenvalue的絕對值是不是都小於1。</p>
<pre>
x = D<sup>-1</sup> [b - (L+U)x]
x = D<sup>-1</sup>b - D<sup>-1</sup>(L+U)x
</pre>
<p>滿足strictly diagonally dominant就保證收斂。不滿足時，可能收斂、也可能不收斂。</p>
<pre>
for each row, |Aii| > ∑ |Aij|
                     j≠i
</pre>
<p class="t">Gauss-Seidel Preconditioner</p>
<p>每回合依序計算x、y、z，剛出爐的數字，馬上拿來使用，加快收斂速度。</p>
<pre>
[ 4  3 -1 ] [ x ]   [ 1 ]
[ 2  5  1 ] [ y ] = [ 2 ]
[-2 -2  6 ] [ z ]   [ 3 ]

{  4x + 3y -  z = 1     { x = (1 - 3y +  z) / 4
{  2x + 5y +  z = 2  => { y = (2 - 2x -  z) / 5
{ -2x - 2y + 6z = 3     { z = (3 + 2x + 2y) / 6

[ x<sub>0</sub> ]   [ 0 ]
[ y<sub>0</sub> ] = [ 0 ] 隨便設定一個初始值
[ z<sub>0</sub> ] = [ 0 ]

[ x<sub>1</sub> ]   [ (1 - 3y<sub>0</sub> +  z<sub>0</sub>) / 4 ]
[ y<sub>1</sub> ] = [ (2 - 2x<sub>1</sub> -  z<sub>0</sub>) / 5 ]
[ z<sub>1</sub> ]   [ (3 + 2x<sub>1</sub> + 2y<sub>1</sub>) / 6 ]
依序計算x<sub>1</sub>、y<sub>1</sub>、z<sub>1</sub>，剛出爐的數字，馬上拿來使用，加快收斂速度。
</pre>
<pre>
x<sub>k+1</sub> = D<sup>-1</sup> (b - Ux<sub>k</sub> - Lx<sub>k+1</sub>)
</pre>
<p class="t">Successive Over Relaxation</p>
<p>原數值、新數值，以固定比例混合。</p>
<pre>
[ x<sub>1</sub> ]   [ (1-w) * x<sub>0</sub> + w * (1 - 3y<sub>0</sub> +  z<sub>0</sub>) / 4 ]
[ y<sub>1</sub> ] = [ (1-w) * y<sub>0</sub> + w * (2 - 2x<sub>1</sub> -  z<sub>0</sub>) / 5 ]
[ z<sub>1</sub> ]   [ (1-w) * z<sub>0</sub> + w * (3 + 2x<sub>1</sub> + 2y<sub>1</sub>) / 6 ]
</pre>

</div></div><div class="a"><div class="h">
<p class="b">Linear Equation Solving: Least Squares</p>
</div><div class="c">
<p class="t">Least Squares</p>
<p>本章節的先備知識是「<a href="Optimization.html">Optimization</a>」。</p>
<p>唯一解是稀奇的，無解、多解是普遍的。無解、多解時，可以採用前面章節Jacobi Predictioner系列的演算法，找到近似解；也可以採用本章節的演算法，找到平方誤差最小的解。</p>
<pre>
solve Ax = b

 overdetermined system: 等式太多 -> 無解 -> 改求‖Ax - b‖²最小的解
underdetermined system: 等式太少 -> 多解 -> 改求‖x‖²最小的解
</pre>
<p>等式太多、等式太少（中學數學的講法是：變數少於等號、變數多於等號），兩種情況分別處理。嚴格來說，必須預先消去所有等價的、多餘的等式，以rank大小、矩陣大小來區分這兩種情況。</p>
<p>一、等式太多因而無解：方程組每一道等式，求得等號左右兩邊的差的平方；累計所有等式，總和越小越好。</p>
<p>二、等式太少因而多解：解的每一項的平方，總和越小越好。</p>
<p>Least意指「盡量小」，Squares意指「平方和」。</p>
<p>平方誤差的優勢是：循規蹈矩，成為一個參考指標，誤差高低可以拿來比較，科學多了。缺陷是：計算速度慢。</p>
<p>平方誤差比起絕對值誤差，有兩個好處：一、讓每道等式的誤差保持均勻，不會有某道等式誤差特別高。二、「一次微分等於零」容易推導數學公式。</p>
<p class="t">三種數學公式</p>
<p>MATLAB按一按，答案就出來了，大可不必深究細節。</p>
<pre>
solve overdetermined system Ax = b   minimize ‖Ax - b‖²

       T    -1  T          T       T
x = ( A  A )   A  b     ( A A x = A b )   Normal Equation

     -1 T
x = R  Q  b             ( A = Q R )       QR Decomposition

        +  T                       T
x = V  Σ  U  b          ( A = U Σ V )     Singular Value Decomposition

     +
x = A  b                                  Pseudoinverse
</pre>
<pre>
solve underdetermined system Ax = b   minimize ‖x‖²

     T      T -1  
x = A  ( A A )   b                        Normal Equation

         T -1              T
x = Q ( R )   b         ( A = Q R )       QR Decomposition

        +  T               T       T
x = U  Σ  V  b          ( A = U Σ V )     Singular Value Decomposition
</pre>
<p class="t">數學公式（Normal Equation）</p>
<p><a href="http://people.csail.mit.edu/bkph/articles/Pseudo_Inverse.pdf">http://people.csail.mit.edu/bkph/articles/Pseudo_Inverse.pdf</a></p>
<p>線性代數最經典的一道公式！這道公式呼應「極值出現在一次微分等於零的地方」。</p>
<p>以下只證明等式太多的情況。時間複雜度O(N^3)。</p>
<pre>
solve  Ax = b
                 2
minimize ‖Ax - b‖

∂          2
―― ‖Ax - b‖ = 0                     極值出現在一次微分等於零
∂x
  ∂
[ ―― (Ax - b) ] [ 2(Ax - b) ] = 0   微分連鎖律
  ∂x
 T
A  [ 2(Ax - b) ] = 0                微分

 T       T
A A x = A b                         同除以2、展開、移項

       T    -1  T  
x = ( A  A )   A  b                 移項。注意到A的向量們必須線性獨立！
</pre>
<p>注意到最後一步，A的向量們必須線性獨立（事先清除冗餘的、無意義的變數），AᵀA才有反矩陣。</p>
<p>矩陣多項式微分，請參考本站文件「<a href="Matrix.html">Matrix Calculus</a>」。</p>
<p class="t">數學公式（QR Decompostion）</p>
<p><a href="http://www.cs.cornell.edu/courses/cs322/2007sp/notes/qr.pdf">http://www.cs.cornell.edu/courses/cs322/2007sp/notes/qr.pdf</a></p>
<p>A = QR。將矩陣拆開成正規正交矩陣Q、零餘部分R。正規正交矩陣Q不影響最小值，最小值取決於零餘部分R。</p>
<p>以下只證明等式太多的情況。等式太少的情況，改為分解A的轉置矩陣Aᵀ = QR。</p>
<p>時間複雜度O(N^3)。但是計算量比Normal Equation少。</p>
<pre>
solve  Ax = b

              2
min ‖ Ax - b ‖

       T          2
min ‖ Q (Ax - b) ‖         正規正交矩陣，變換後長度不變

       T       T   2
min ‖ Q A x - Q b ‖        展開

             T   2          T     T
min ‖ R x - Q b ‖          Q A = Q Q R = R

                 T     2
    ‖ [ R₁ x - Q₁ b ] ‖    R = [ R₁ ]  區分出零，讓R₁是方陣
min ‖ [          T  ] ‖        [ 0  ]  區分上段和下段
    ‖ [    0 - Q₂ b ] ‖ 

            T
令 R₁ x - Q₁ b = 0         此式有唯一解，可為零
               T   2
令最小值是 ‖ Q₂ b ‖ 

         T
R₁ x = Q₁ b                移項

     -1  T
x = R₁ Q₁ b                移項
</pre>
<p class="e">Timus 1668</p>
<p class="t">數學公式（Singular Value Decompostion）</p>
<p>和QR分解的手法如出一轍。</p>
<p>以下只證明等式太多的情況。等式太少的情況，改為分解A的轉置矩陣Aᵀ = UΣVᵀ。</p>
<p>時間複雜度O(N^3 + NK)。我不確定實務上是否比較快。</p>
<pre>
solve  Ax = b

              2
min ‖ Ax - b ‖

       T          2
min ‖ U (Ax - b) ‖       正規正交矩陣，變換後長度不變

       T       T   2
min ‖ U A x - U b ‖      展開

         T     T   2      T     T     T      T
min ‖ Σ V x - U b ‖      U A = U U Σ V  = Σ V 

      T     T
令 Σ V x - U b = 0       此式有唯一解，可為零

   T     T
Σ V x = U b              移項

        +  T
x = V  Σ  U  b           移項
</pre>

</div></div><div class="a"><div class="h">
<p class="b">Linear Equation Solving: Regularization</p>
</div><div class="c">
<p class="t">Regularization</p>
<p>本章節的先備知識是「<a href="Optimization.html">Constrained Optimization</a>」。</p>
<p>線性方程組，無解、多解時，我們增加限制條件，以得到唯一解。以下以無解為例。</p>
<pre>
                                  2
solve Ax = b     minimize ‖Ax - b‖ 
</pre>
<p>我們可以再添加其他限制條件。</p>
<pre>
                                  2
solve Ax = b     minimize ‖Ax - b‖      subject to f(x) ≥ 0
</pre>
<p>根據Lagrange Multiplier，限制條件可以併入最佳化的對象。</p>
<pre>
                                  2
solve Ax = b     minimize ‖Ax - b‖ + α f(x)   (α ≥ 0)
</pre>
<p>α理應是未知數，不過此處改成了一個自訂數值。我們視問題需要，訂立適當數值。數值越小，限制條件的影響力就越小，類似於加權平均的概念。</p>
<p>求最小值，權重的絕對大小不重要，考慮相對大小即可。我們習慣把第一個限制的權重定為1，節省一個權重數值。</p>
<p>f(x)稱做loss function，應該是「損失越少越好」的意思。我們視問題需要，訂立適當函數，例如x的長度越小越好f(x) = ‖x‖；x的長度越長越好f(x) = -‖x‖；物理學的做功越少越好f(x) = F x。</p>
<p>這稱做「正則化」。明定準則、施予規範。</p>
<p class="t">Tikhonov Regularization</p>
<p>線性方程組，有許多式子和變數。可能有其中一群變數與式子構成無解、另一群構成唯一解、剩下一群構成多解。更有甚者，切割一些群結果無解變多解、整併某些群結果多解變無解。</p>
<p>無法釐清是無解、多解的時候，那就兩個限制一起上吧。</p>
<pre>
                                   2        2
solve Ax = b      minimize ‖Ax - b‖  + α ‖x‖    (α ≥ 0)
</pre>
<pre>
∂            2        2        
―― [ ‖Ax - b‖  + α ‖x‖  ] = 0    極值出現於「一次微分等於零」的地方
∂x                               二次函數、恆正，此時極值就是最小值
   T         T
2 A A x - 2 A b + 2 α x = 0      展開

   T               T                             T
( A A + α I ) x = A b            移項，左式即是 A A 的對角線加上 α
</pre>
<p>左式是實數對稱正定方陣，有唯一解。時間複雜度O(N^3)。</p>
<p class="t">Homogeneous Linear Equations</p>
<p>討論特例b = 0的情況。當b = 0，則x = 0，缺乏討論意義。於是添加限制「x長度（的平方）為1」，增進討論意義。</p>
<pre>
solve Ax = 0
             2
minimize ‖Ax‖
              2
subject to ‖x‖ = 1
</pre>
<pre>
             2         2
minimize ‖Ax‖ - λ ( ‖x‖ - 1 )     Lagrange multiplier

∂        2         2
―― [ ‖Ax‖ - λ ( ‖x‖ - 1 ) ] = 0   極值出現於「一次微分等於零」的地方
∂x
   T
2 A A x - 2 λ x = 0               展開

 T
A A x = λ x                       移項，此即特徵向量的格式
</pre>
<p>答案是AᵀA的最小的特徵值的特徵向量！又因為AᵀA是實數對稱正半定方陣，所以特徵值都是正數、零。</p>
<p>欲求最小的特徵值，可以採用QR Iteration或Lanczos Iteration演算法求得所有特徵值，再挑出最小的，時間複雜度O(N^3 + N^2 * K)。亦可採用Singular Value Decomposition的演算法，不必計算AᵀA，節省一點時間。</p>

</div></div><div class="a"><div class="h">
<p class="b">Linear Equation Solving: Optimization（Under Construction!）</p>
</div><div class="c">
<p class="t">Conjugate Gradient Method</p>
<p>採用最佳化演算法Gradient Method，針對平方誤差進行改良，速度更快。</p>
<p>平方誤差的矩陣形式，即是對稱正定矩陣。</p>
<pre>
http://www.cs.ucsb.edu/~gilbert/cs219/cs219Spr2013/Slides/cs219-CgIntro.pptx
http://graphics.stanford.edu/courses/cs205a-15-spring/assets/lecture_slides/cg_i.pdf
</pre>
<p class="t">Gauss-Newton Algorithm</p>
<p>採用最佳化演算法Newton Method，針對平方誤差進行改良，速度更快。</p>
<p class="t">Levenberg-Marquardt Algorithm</p>
<p>視情況使用Conjugate Gradient Method或者Gauss-Newton Algorithm，兩害相權取其輕。</p>

</div></div><div class="a"><div class="h">
<p class="b">Linear Inequalities Solving</p>
</div><div class="c">
<p class="t">Linear Inequalities</p>
<p>線性不等式組。許多道線性不等式同時成立。</p>
<p>正是計算幾何「<a href="Half-planeIntersection.html">Half-plane Intersection</a>」推廣到高維度，所有解形成一個凸多胞形，也可能形成開放區間、退化、空集合。</p>
<p>目前沒有演算法。大家習慣採用「<a href="Optimization.html">Linear Programming</a>」，將凸多胞形硬是位移至第一象限（各個變數加上一個足夠大的數值，代換成新變數），以符合線性規劃的格式。</p>
<p>也有人適度乘上負號，調整成Ax > b的格式，套用高斯消去法，但是不知道正不正確。</p>
</div></div><script src="h.js"></script></body></html>